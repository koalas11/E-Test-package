{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2: Comparative Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [FAST++](#fast)\n",
    "- [Field-ready testing](#field-ready-testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from statistics import mean\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"similar\", \"fixed\", \"buggy\"]\n",
    "FORMAL_LABELS = [\"already-tested\", \"need-test\", \"error-prone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_effectiveness(df_exp: pd.DataFrame):\n",
    "    y_true = df_exp[\"target_scenario\"].array\n",
    "    y_pred = df_exp[\"classified_scenario\"].array\n",
    "    precision = precision_score(y_true, y_pred, labels=LABELS, average=None)\n",
    "    recall = recall_score(y_true, y_pred, labels=LABELS, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, labels=LABELS, average=None)\n",
    "    not_yet_tested_f1 = f1[1:].mean()\n",
    "    avg_f1 = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n",
    "    avg_precision = precision_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n",
    "    avg_recall = recall_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            [f\"{l}(P R F1)\" for l in FORMAL_LABELS]\n",
    "            + [\"not-yet-tested F1\", \"Total Average F1\"]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \" & \".join(\n",
    "            [\n",
    "                \" & \".join([f\"{precision[i]:.2f}\", f\"{recall[i]:.2f}\", f\"{f1[i]:.2f}\"])\n",
    "                for i in range(len(LABELS))\n",
    "            ]\n",
    "            + [f\"{not_yet_tested_f1:.2f}\", f\"{avg_f1:.2f}\"]\n",
    "        )\n",
    "    )\n",
    "    print(f\"avg_precision: {avg_precision:.2f}, avg_recall: {avg_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST++\n",
    "Run following commands to reproduce FAST++ classification:\n",
    "```sh\n",
    "cd DataAnalysis/\n",
    "conda create -n venv_fastr python=3.6 -c conda-forge --no-default-packages\n",
    "conda activate venv_fastr\n",
    "unset PYTHONPATH\n",
    "python fastr_classify.py\n",
    "# replace RESULTS_FILE_PATH with the generated one\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already-tested(P R F1) | need-test(P R F1) | error-prone(P R F1) | not-yet-tested F1 | Total Average F1\n",
      "0.32 & 0.32 & 0.32 & 0.27 & 0.27 & 0.27 & 0.44 & 0.44 & 0.44 & 0.35 & 0.34\n",
      "avg_precision: 0.34, avg_recall: 0.34\n"
     ]
    }
   ],
   "source": [
    "RESULTS_FILE_PATH = \"plotdata/fastr_classification_20250121_101917.csv\"\n",
    "df_results = pd.read_csv(RESULTS_FILE_PATH)\n",
    "compute_effectiveness(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field-ready testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication package: https://github.com/field-ready-test-cases/field-ready-test-cases/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_RESULTS_PATH = \"../AutonomicTester/experiment_results/finetuning/FineTunedGPT3.5Turbo/20240715_215858_OpenAI GPT-3.5 Fine-tuned_Validation_buggy\"\n",
    "PROJECTS = [\"Chart\", \"Lang\"]\n",
    "SCENARIO_VOTES_PATH = os.path.join(EXPERIMENT_RESULTS_PATH, \"scenario_votes.csv\")\n",
    "df_votes = pd.read_csv(SCENARIO_VOTES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 bugs out of 25 bugs in Chart\n",
      "Percentage: 0.48\n"
     ]
    }
   ],
   "source": [
    "df_votes_chart = df_votes.loc[(df_votes[\"project\"] == \"Chart\")]\n",
    "num_found_bugs_chart = len(df_votes_chart.loc[df_votes_chart[\"scenario\"] == \"buggy\"])\n",
    "num_total_bugs_chart = len(df_votes_chart)\n",
    "print(f\"Found {num_found_bugs_chart} bugs out of {num_total_bugs_chart} bugs in Chart\")\n",
    "percent_revealed_bugs_chart = num_found_bugs_chart / num_total_bugs_chart\n",
    "print(f\"Percentage: {percent_revealed_bugs_chart:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 bugs out of 59 bugs in Lang\n",
      "Percentage: 0.54\n"
     ]
    }
   ],
   "source": [
    "df_votes_lang = df_votes.loc[(df_votes[\"project\"] == \"Lang\")]\n",
    "num_found_bugs_lang = len(df_votes_lang.loc[df_votes_lang[\"scenario\"] == \"buggy\"])\n",
    "num_total_bugs_lang = len(df_votes_lang)\n",
    "print(f\"Found {num_found_bugs_lang} bugs out of {num_total_bugs_lang} bugs in Lang\")\n",
    "percent_revealed_bugs_lang = num_found_bugs_lang / num_total_bugs_lang\n",
    "print(f\"Percentage: {percent_revealed_bugs_lang:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 bugs out of 84 bugs in total\n",
      "Percentage: 0.52\n"
     ]
    }
   ],
   "source": [
    "df_votes_subset = df_votes.loc[df_votes[\"project\"].isin(PROJECTS)]\n",
    "num_found_bugs_subset = len(df_votes_subset.loc[df_votes_subset[\"scenario\"] == \"buggy\"])\n",
    "num_total_bugs_subset = len(df_votes_subset)\n",
    "print(\n",
    "    f\"Found {num_found_bugs_subset} bugs out of {num_total_bugs_subset} bugs in total\"\n",
    ")\n",
    "percent_revealed_bugs_subset = num_found_bugs_subset / num_total_bugs_subset\n",
    "print(f\"Percentage: {percent_revealed_bugs_subset:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAULT_DENSITY = 0.00100\n",
    "NUM_INPUTS = int((26 + 65) / 0.00100)\n",
    "REVEALED_FAILURES = {\n",
    "    \"#faults\": [26, 65],\n",
    "    \"#executed faults\": [16, 42],\n",
    "    \"revealed failures\": [10, 20],\n",
    "}\n",
    "FIELD_TEST_TRIGGERING_PERCENTAGE = 0.00141\n",
    "NUM_BUGGY = 26 + 65\n",
    "NUM_FIXED = int((NUM_INPUTS - NUM_BUGGY) / 2)\n",
    "NUM_SIMILAR = NUM_INPUTS - NUM_FIXED - NUM_BUGGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    \"target_scenario\": [\"buggy\"] * NUM_BUGGY\n",
    "    + [\"fixed\"] * NUM_FIXED\n",
    "    + [\"similar\"] * NUM_SIMILAR,\n",
    "    \"classified_scenario\": [\"buggy\"] * 30 + [\"similar\"] * (NUM_INPUTS - 30),\n",
    "}\n",
    "df = pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df: pd.DataFrame):\n",
    "    if df[\"target_scenario\"] == \"buggy\" and df[\"classified_scenario\"] == \"similar\":\n",
    "        if random.random() <= FIELD_TEST_TRIGGERING_PERCENTAGE:\n",
    "            df[\"classified_scenario\"] = \"fixed\"\n",
    "    elif df[\"target_scenario\"] == \"fixed\":\n",
    "        if random.random() <= FIELD_TEST_TRIGGERING_PERCENTAGE:\n",
    "            df[\"classified_scenario\"] = \"buggy\"\n",
    "    elif df[\"target_scenario\"] == \"similar\":\n",
    "        if random.random() <= FIELD_TEST_TRIGGERING_PERCENTAGE:\n",
    "            df[\"classified_scenario\"] = \"buggy\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ketaiqiu/Projects/autonomic-testing/E-Test-package/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already-tested(P R F1) | need-test(P R F1) | error-prone(P R F1) | not-yet-tested F1 | Total Average F1\n",
      "0.50 & 1.00 & 0.67 & 0.00 & 0.00 & 0.00 & 0.16 & 0.33 & 0.22 & 0.11 & 0.30\n",
      "avg_precision: 0.22, avg_recall: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ketaiqiu/Projects/autonomic-testing/E-Test-package/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "classified_df = df.apply(classify, axis=1)\n",
    "compute_effectiveness(classified_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
